{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-d349604b-7056-49bb-42cf-c9d6fefab40a)\n",
      "GPU 1: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-cb3041a3-8c48-0cbe-84ec-65646db1d19f)\n",
      "GPU 2: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-900b1eb3-d0c3-b27b-cec8-0c31c86bf0e9)\n",
      "GPU 3: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-1dbe66f4-c08c-4abe-14b9-af6c71af8407)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L\n",
    "#!CUDA_VISIBLE_DEVICES=MIG-00992b28-9de1-5e60-993b-b31261914ee9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRFL_images = load_dataset(\"lampent/IRFL\", data_files='IRFL_images.zip')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>figurative_type</th>\n",
       "      <th>source</th>\n",
       "      <th>uuid</th>\n",
       "      <th>category</th>\n",
       "      <th>theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>https://www.israelhayom.com/2022/03/01/ukraini...</td>\n",
       "      <td>1009925977381951166573538219201192200312184397...</td>\n",
       "      <td>Figurative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>https://www.army.mil/article/260659/soldier_fo...</td>\n",
       "      <td>1110158887118147260462183682336548897929003447...</td>\n",
       "      <td>Figurative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>https://www.spokesman.com/stories/2012/aug/03/...</td>\n",
       "      <td>1150022408148171978092024654350648179128902573...</td>\n",
       "      <td>Figurative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>https://www.theguardian.com/books/2016/feb/26/...</td>\n",
       "      <td>4334616426942719694991676075800911426738633635...</td>\n",
       "      <td>Figurative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>https://www.polygon.com/22396791/battlefield-6...</td>\n",
       "      <td>6641158141547602068103775945956449252322278422...</td>\n",
       "      <td>Figurative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      phrase figurative_type  \\\n",
       "0  a lion on the battlefield        metaphor   \n",
       "1  a lion on the battlefield        metaphor   \n",
       "2  a lion on the battlefield        metaphor   \n",
       "3  a lion on the battlefield        metaphor   \n",
       "4  a lion on the battlefield        metaphor   \n",
       "\n",
       "                                              source  \\\n",
       "0  https://www.israelhayom.com/2022/03/01/ukraini...   \n",
       "1  https://www.army.mil/article/260659/soldier_fo...   \n",
       "2  https://www.spokesman.com/stories/2012/aug/03/...   \n",
       "3  https://www.theguardian.com/books/2016/feb/26/...   \n",
       "4  https://www.polygon.com/22396791/battlefield-6...   \n",
       "\n",
       "                                                uuid    category  theme  \n",
       "0  1009925977381951166573538219201192200312184397...  Figurative    NaN  \n",
       "1  1110158887118147260462183682336548897929003447...  Figurative    NaN  \n",
       "2  1150022408148171978092024654350648179128902573...  Figurative    NaN  \n",
       "3  4334616426942719694991676075800911426738633635...  Figurative    NaN  \n",
       "4  6641158141547602068103775945956449252322278422...  Figurative    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRFL_metaphors_dataset = load_dataset(\"lampent/IRFL\", 'metaphors-dataset')['dataset']\n",
    "pd.DataFrame(IRFL_metaphors_dataset).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function for local run\n",
    "def get_image_path_from_folder(image_name):\n",
    "  image_folder_path = r'C:\\devel\\IRLM\\assets\\D_images'\n",
    "  return f'{image_folder_path}\\\\{image_name.split(\".\")[0] + \".jpeg\"}'\n",
    "\n",
    "def get_image_path_from_hugginface_cache(image_name):\n",
    "    chached_image_path = IRFL_images[0]['image'].filename\n",
    "    chached_image_name = chached_image_path.split('/')[-1]\n",
    "    return chached_image_path.replace(chached_image_name, str(image_name).split('.')[0] + '.jpeg')\n",
    "\n",
    "def get_image(image_name):\n",
    "  image_path = get_image_path_from_hugginface_cache(image_name)\n",
    "  return Image.open(image_path)\n",
    "\n",
    "# Preference task\n",
    "IRFL_idiom_retrieval_task = load_dataset(\"lampent/IRFL\", 'idiom-retrieval-task')[\"test\"]\n",
    "IRFL_metaphor_retrieval_task = load_dataset(\"lampent/IRFL\", 'metaphor-retrieval-task')[\"test\"]\n",
    "IRFL_simile_retrieval_task = load_dataset(\"lampent/IRFL\", 'simile-retrieval-task')[\"test\"]\n",
    "IRFL_open_simile_retrieval_task = load_dataset(\"lampent/IRFL\", 'open-simile-retrieval-task')[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>figurative_type</th>\n",
       "      <th>images_metadata</th>\n",
       "      <th>first_category</th>\n",
       "      <th>second_category</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FvsPL</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>[{\"source\": \"https://www.digitaltrends.com/gam...</td>\n",
       "      <td>[\"10099259773819511665735382192011922003121843...</td>\n",
       "      <td>[\"11078326901046011968447681574355317880314753...</td>\n",
       "      <td>a lion on the battlefield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FvsPL</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>[{\"source\": \"https://www.dreamstime.com/photos...</td>\n",
       "      <td>[\"10166014906044208703673588942775925697155165...</td>\n",
       "      <td>[\"10472048254956553018220152788477942861449900...</td>\n",
       "      <td>a mighty lion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FvsPL</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>[{\"source\": \"https://unsplash.com/s/photos/nig...</td>\n",
       "      <td>[\"10238828137439183291164181490873662971856351...</td>\n",
       "      <td>[\"10441871388007842228188677322605006948277838...</td>\n",
       "      <td>a night owl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FvsPL</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>[{\"source\": \"https://solarsystem.nasa.gov/moon...</td>\n",
       "      <td>[\"10225631305623393414613139653908189397782521...</td>\n",
       "      <td>[\"10975358208463655666118304845299235749355170...</td>\n",
       "      <td>a shinning star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FvsPL</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>[{\"source\": \"https://www.istockphoto.com/photo...</td>\n",
       "      <td>[\"13412256301354044503872943123661426267175623...</td>\n",
       "      <td>[\"11009120221646905702971515855412252245336570...</td>\n",
       "      <td>blanket of bullets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type figurative_type                                    images_metadata  \\\n",
       "0  FvsPL        metaphor  [{\"source\": \"https://www.digitaltrends.com/gam...   \n",
       "1  FvsPL        metaphor  [{\"source\": \"https://www.dreamstime.com/photos...   \n",
       "2  FvsPL        metaphor  [{\"source\": \"https://unsplash.com/s/photos/nig...   \n",
       "3  FvsPL        metaphor  [{\"source\": \"https://solarsystem.nasa.gov/moon...   \n",
       "4  FvsPL        metaphor  [{\"source\": \"https://www.istockphoto.com/photo...   \n",
       "\n",
       "                                      first_category  \\\n",
       "0  [\"10099259773819511665735382192011922003121843...   \n",
       "1  [\"10166014906044208703673588942775925697155165...   \n",
       "2  [\"10238828137439183291164181490873662971856351...   \n",
       "3  [\"10225631305623393414613139653908189397782521...   \n",
       "4  [\"13412256301354044503872943123661426267175623...   \n",
       "\n",
       "                                     second_category  \\\n",
       "0  [\"11078326901046011968447681574355317880314753...   \n",
       "1  [\"10472048254956553018220152788477942861449900...   \n",
       "2  [\"10441871388007842228188677322605006948277838...   \n",
       "3  [\"10975358208463655666118304845299235749355170...   \n",
       "4  [\"11009120221646905702971515855412252245336570...   \n",
       "\n",
       "                      phrase  \n",
       "0  a lion on the battlefield  \n",
       "1              a mighty lion  \n",
       "2                a night owl  \n",
       "3            a shinning star  \n",
       "4         blanket of bullets  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(IRFL_metaphor_retrieval_task).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import ast, os\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "detector = torchvision.models.resnet50(pretrained=True)\n",
    "detector = torch.nn.Sequential(*list(detector.children())[:-1])\n",
    "detector.eval()\n",
    "\n",
    "batch_size = 128\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "def get_data(task_name):\n",
    "    figurative_images = [ast.literal_eval(image_list) for image_list in task_name['first_category']]\n",
    "    partial_literal_images = [ast.literal_eval(image_list) for image_list in task_name['second_category']]\n",
    "    phrases = task_name['phrase']\n",
    "\n",
    "    figurative_dictionary = []\n",
    "    for idx, element in enumerate(figurative_images):\n",
    "        for image in element:\n",
    "            dictionary = {'phrase': phrases[idx], 'image': image, 'type': 'figurative'}\n",
    "            figurative_dictionary.append(dictionary)\n",
    "        \n",
    "    for idx, element in enumerate(partial_literal_images):\n",
    "        for image in element:\n",
    "            dictionary = {'phrase': phrases[idx], 'image': image, 'type': 'partial literal'}\n",
    "            figurative_dictionary.append(dictionary)\n",
    "    \n",
    "    return figurative_dictionary\n",
    "\n",
    "metaphor_data = get_data(IRFL_metaphor_retrieval_task)\n",
    "idiom_data = get_data(IRFL_idiom_retrieval_task)\n",
    "simile_data = get_data(IRFL_simile_retrieval_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_table(dictionary_list, data_kind):\n",
    "    column_names=[\"phrase\", \"image\", \"type\"]\n",
    "    \n",
    "    df = pd.DataFrame(dictionary_list, columns=column_names)\n",
    "    filepath =  'data/' + data_kind + '.csv'\n",
    "    file = df.to_csv(filepath, index=False)\n",
    "\n",
    "create_data_table(metaphor_data, \"metaphor_data\")\n",
    "create_data_table(idiom_data, \"idiom_data\")\n",
    "create_data_table(simile_data, \"simile_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>image</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>1009925977381951166573538219201192200312184397...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>1110158887118147260462183682336548897929003447...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>1150022408148171978092024654350648179128902573...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>4334616426942719694991676075800911426738633635...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a lion on the battlefield</td>\n",
       "      <td>6641158141547602068103775945956449252322278422...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      phrase  \\\n",
       "0  a lion on the battlefield   \n",
       "1  a lion on the battlefield   \n",
       "2  a lion on the battlefield   \n",
       "3  a lion on the battlefield   \n",
       "4  a lion on the battlefield   \n",
       "\n",
       "                                               image        type  \n",
       "0  1009925977381951166573538219201192200312184397...  figurative  \n",
       "1  1110158887118147260462183682336548897929003447...  figurative  \n",
       "2  1150022408148171978092024654350648179128902573...  figurative  \n",
       "3  4334616426942719694991676075800911426738633635...  figurative  \n",
       "4  6641158141547602068103775945956449252322278422...  figurative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/metaphor_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image):\n",
    "    vis_embeddings = detector(image.unsqueeze(0))\n",
    "    return vis_embeddings\n",
    "\n",
    "class Multimodal_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        data = pd.read_csv(data)\n",
    "        self.phrases = data['phrase']\n",
    "        self.images = data['image']\n",
    "        self.labels = data['type']\n",
    "\n",
    "        transform_list = [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phrases)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        encodings = tokenizer.encode_plus(self.phrases[idx], add_special_tokens=True, max_length=12, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        image = self.images[idx]\n",
    "\n",
    "        label = 0 if self.labels[idx] == 'figurative' else 1\n",
    "\n",
    "        img = get_image(image)\n",
    "        img = img.convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        embeddings = get_features(img)\n",
    "\n",
    "        item = {'phrase': self.phrases[idx], 'input_ids': encodings['input_ids'], 'attn_mask': encodings['attention_mask'], 'token_type_ids': encodings['token_type_ids'], 'visual_embeddings': embeddings, 'type': label}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = Multimodal_Dataset('data/simile_data.csv', tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    ids = batch['visual_embeddings']\n",
    "    print(ids.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VISUAL_BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VISUAL_BERT, self).__init__()\n",
    "        self.visual_bert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "        self.classifier = nn.Linear(self.visual_bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids, visual_embeddings):\n",
    "        \n",
    "        visual_token_type_ids = torch.ones(visual_embeddings.shape[:-1], dtype=torch.long).to(device)\n",
    "        visual_attention_mask = torch.ones(visual_embeddings.shape[:-1], dtype=torch.float).to(device)\n",
    "        \n",
    "        outputs = self.visual_bert(input_ids=input_ids, attention_mask=attn_masks, token_type_ids=token_type_ids, visual_embeds=visual_embeddings, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
    "        predictions = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [05:28<00:00,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8294984085219247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = VISUAL_BERT().to(device)\n",
    "model.eval()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "total_loss = 0\n",
    "predictions_visual = []\n",
    "gold_labels_visual = []\n",
    "losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = torch.Tensor(batch['input_ids']).long().to(device)\n",
    "        \n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).float().to(device)\n",
    "        token_type_ids = torch.Tensor(batch['token_type_ids']).long().to(device)\n",
    "        \n",
    "        visual_embeddings = torch.Tensor(batch['visual_embeddings']).float().to(device)\n",
    "        visual_embeddings = visual_embeddings.squeeze(3)\n",
    "        visual_embeddings = visual_embeddings.squeeze(3).to(device)\n",
    "        # print(input_ids.squeeze().size())\n",
    "        # print(visual_embeddings.size())\n",
    "        # print(attn_masks.size())\n",
    "        # print(token_type_ids.size())\n",
    "        gold_label = batch['type'].to(device)\n",
    "\n",
    "        outputs = model(input_ids.squeeze(1), attn_masks.squeeze(1), token_type_ids.squeeze(1), visual_embeddings)\n",
    "\n",
    "        gold_labels_visual.extend(gold_label.cpu().numpy())\n",
    "        \n",
    "        loss = loss_fn(outputs, gold_label)\n",
    "        losses.append(loss)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        \n",
    "        predictions_visual.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "average_loss = total_loss / len(dataloader)\n",
    "print(f'Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40      1107\n",
      "           1       0.00      0.00      0.00      3314\n",
      "\n",
      "    accuracy                           0.25      4421\n",
      "   macro avg       0.13      0.50      0.20      4421\n",
      "weighted avg       0.06      0.25      0.10      4421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(gold_labels_visual, predictions_visual)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 2214\n",
      "total randomized words: 2214\n",
      "total training set: 1771\n",
      "total testing set: 443\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(csv_file):\n",
    "    \n",
    "    file = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(\"total words:\",len(file))\n",
    "    randomized_data = file.sample(frac=1, random_state=42)\n",
    "    print(\"total randomized words:\",len(randomized_data))\n",
    "\n",
    "    train = int(len(randomized_data) * 0.8)\n",
    "\n",
    "    train_data = randomized_data[:train]\n",
    "    print(\"total training set:\",len(train_data))\n",
    "\n",
    "    test_data = randomized_data[train:]\n",
    "    print(\"total testing set:\",len(test_data))\n",
    "\n",
    "    train_filepath = \"data/train_dataset.csv\"\n",
    "    test_filepath = \"data/test_dataset.csv\"\n",
    "    train_data.to_csv(train_filepath, index=False)\n",
    "    test_data.to_csv(test_filepath, index=False)\n",
    "\n",
    "split_dataset('data/balanced_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Multimodal_Dataset('data/train_dataset.csv', tokenizer=tokenizer)\n",
    "test_data = Multimodal_Dataset('data/test_dataset.csv', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    figurative = 0\n",
    "    partial_literal = 0\n",
    "    for label in df['type'].to_list():\n",
    "        if label == 'figurative':\n",
    "            figurative += 1\n",
    "        else:\n",
    "            partial_literal += 1\n",
    "\n",
    "    print(' Number of figurative instances:', figurative, '\\n','Number of partial literal instances:', partial_literal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of figurative instances: 894 \n",
      " Number of partial literal instances: 877\n"
     ]
    }
   ],
   "source": [
    "count_instances('data/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    figurative = df.head(1107)\n",
    "    literal = df.tail(1107)\n",
    "    balanced_dataset = pd.concat([figurative, literal])\n",
    "    balanced_dataset.to_csv('data/balanced_dataset.csv', index=False)\n",
    "\n",
    "balance_data('data/simile_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.6983288483960288\r"
     ]
    }
   ],
   "source": [
    "language_and_vision_model = VISUAL_BERT().to(device)\n",
    "epochs = 2\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(language_and_vision_model.parameters(), lr=0.001)\n",
    "\n",
    "language_and_vision_model.train()\n",
    "\n",
    "losses_train = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = torch.LongTensor(batch['input_ids']).to(device)\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).float().to(device)\n",
    "        #print(type(attn_masks))\n",
    "        token_type_ids = torch.LongTensor(batch['token_type_ids']).to(device)\n",
    "        \n",
    "        visual_embeddings = torch.FloatTensor(batch['visual_embeddings']).to(device)\n",
    "        \n",
    "        visual_embeddings = visual_embeddings.squeeze(3)\n",
    "        visual_embeddings = visual_embeddings.squeeze(3)\n",
    "        \n",
    "        gold_label = batch['type'].to(device)\n",
    "    \n",
    "        outputs = language_and_vision_model(input_ids.squeeze(), attn_masks.squeeze(), token_type_ids.squeeze(), visual_embeddings)\n",
    "\n",
    "        loss = loss_fn(outputs, gold_label)\n",
    "        losses_train.append(loss.detach().cpu().numpy())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(\"epoch:\",epoch, \"loss:\", total_loss/(i+1), end='\\r')\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f05b4c86c90>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.20s/it]\n"
     ]
    }
   ],
   "source": [
    "language_and_vision_model.eval()\n",
    "\n",
    "gold_labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids = torch.LongTensor(batch['input_ids']).to(device)\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).float().to(device)\n",
    "        token_type_ids = torch.LongTensor(batch['token_type_ids']).to(device)\n",
    "        \n",
    "        visual_embeddings = torch.FloatTensor(batch['visual_embeddings']).to(device)\n",
    "        visual_embeddings = visual_embeddings.squeeze(3)\n",
    "        visual_embeddings = visual_embeddings.squeeze(3)\n",
    "        # print(input_ids.size())\n",
    "        # print(visual_embeddings.size())\n",
    "        # print(attn_masks.size())\n",
    "        # print(token_type_ids.size())\n",
    "        gold_label = batch['type'].to(device)\n",
    "\n",
    "        outputs = language_and_vision_model(input_ids.squeeze(), attn_masks.squeeze(), token_type_ids.squeeze(), visual_embeddings)\n",
    "\n",
    "        gold_labels.extend(gold_label.cpu().numpy())\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        \n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "gold_labels = numpy.array(gold_labels)\n",
    "predictions = numpy.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       213\n",
      "           1       0.52      1.00      0.68       230\n",
      "\n",
      "    accuracy                           0.52       443\n",
      "   macro avg       0.26      0.50      0.34       443\n",
      "weighted avg       0.27      0.52      0.35       443\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(gold_labels, predictions)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 28.7MB/s]\n",
      "Downloading data: 100%|██████████| 490M/490M [00:28<00:00, 17.2MB/s] \n",
      "Downloading data: 100%|██████████| 464M/464M [00:32<00:00, 14.4MB/s] \n",
      "Downloading data: 100%|██████████| 472M/472M [00:33<00:00, 14.0MB/s] \n",
      "Downloading data: 100%|██████████| 464M/464M [00:29<00:00, 15.7MB/s] \n",
      "Downloading data: 100%|██████████| 475M/475M [00:28<00:00, 16.6MB/s] \n",
      "Downloading data: 100%|██████████| 470M/470M [00:29<00:00, 15.9MB/s] \n",
      "Downloading data: 100%|██████████| 478M/478M [00:28<00:00, 16.5MB/s] \n",
      "Downloading data: 100%|██████████| 486M/486M [00:35<00:00, 13.8MB/s] \n",
      "Downloading data: 100%|██████████| 423M/423M [00:32<00:00, 12.9MB/s] \n",
      "Downloading data: 100%|██████████| 413M/413M [00:29<00:00, 13.9MB/s] \n",
      "Downloading data: 100%|██████████| 426M/426M [00:29<00:00, 14.5MB/s] \n",
      "Generating train split: 100%|██████████| 75750/75750 [17:52<00:00, 70.65 examples/s] \n",
      "Generating validation split: 100%|██████████| 25250/25250 [05:21<00:00, 78.61 examples/s] \n"
     ]
    }
   ],
   "source": [
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x511>,\n",
       " 'label': 10}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AICS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
